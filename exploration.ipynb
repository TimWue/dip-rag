{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import (\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://search.dip.bundestag.de/api/v1\"\n",
    "api_key = \"I9FKdCn.hbfefNWCY336dL6x62vfwNKpoN2RZ1gp21\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all documents metadata\n",
    "metadata_endpoint = \"plenarprotokoll\"\n",
    "headers = { \"Authorization\": f\"ApiKey {api_key}\"}\n",
    "\n",
    "metadata_url = f\"{base_url}/{metadata_endpoint}\"\n",
    "metadatas = requests.get(metadata_url, headers=headers).json()[\"documents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Text of one document\n",
    "fulltext_endpoint = \"plenarprotokoll-text\"\n",
    "params = {\"format\": \"json\"}\n",
    "def get_text(document_id: str) -> str:\n",
    "    fulltext_url = f\"{base_url}/{fulltext_endpoint}/{document_id}\"\n",
    "    document = requests.get(fulltext_url, headers=headers, params=params).json()\n",
    "    return document[\"text\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/projects/plenar-rag/.venv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Create ParentDocumentRetriever\n",
    "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"split_parents\", embedding_function=HuggingFaceEmbeddings(model_name=embedding_model)\n",
    ")\n",
    "store = InMemoryStore()\n",
    "\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_langchain_document_from_protocol_id(protocol_id: str) -> Document:\n",
    "    metadata_url = f\"{base_url}/{metadata_endpoint}/{protocol_id}\"\n",
    "    metadata = requests.get(metadata_url, headers=headers).json()\n",
    "    \n",
    "\n",
    "    retrieval_metadata = {\n",
    "        \"dokument_id\": protocol_id,\n",
    "        \"dokument_art\": metadata[\"dokumentart\"],\n",
    "        \"dokument_nummer\": metadata[\"dokumentnummer\"],\n",
    "        \"titel\": metadata[\"titel\"],\n",
    "        \"datum\": metadata[\"datum\"],\n",
    "    }\n",
    "    retrieval_text = get_text(document_id=protocol_id)\n",
    "\n",
    "    return Document(page_content=retrieval_text, metadata=retrieval_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest one protocol\n",
    "index = 10\n",
    "protocol_id = metadatas[index][\"id\"]\n",
    "\n",
    "document = get_langchain_document_from_protocol_id(protocol_id=protocol_id)\n",
    "retriever.add_documents(documents=[document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/tim/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Load Generation Model\n",
    "llama_model_3 = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "mixtral_model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "llama_2b_70b = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "openai_gpt_3_5 = \"gpt-3.5-turbo\"\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=mixtral_model,\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=1024,\n",
    ")\n",
    "\n",
    "chat_model = ChatOpenAI(model=openai_gpt_3_5)\n",
    "#chat_model = ChatHuggingFace(llm=llm)\n",
    "fact_check_model = ChatHuggingFace(llm=llm)\n",
    "#chat_model = OllamaLLM(model=\"mistral\")\n",
    "#fact_check_model = OllamaLLM(model=\"llama3.2:1b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(documents: list[Document]) -> str:\n",
    "    return \"\\n\\n\".join(document.page_content for document in documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rag_answer(prompt: str):\n",
    "    documents = retriever.invoke(prompt, k=4)\n",
    "    context = format_docs(documents=documents)\n",
    "\n",
    "    simple_rag_prompt = (\n",
    "        \"Beantworte die Frage mit Hilfe der folgenden Kontextinformationen. \"\n",
    "        \"Wenn du die Antwort nicht weißt, sag einfach, dass du die Antwort nicht kennst. \"\n",
    "        \"Verwende zu Beantwortung nur die Informationen im Kontext. Verwende kein externes Wissen. \"\n",
    "        \"Verwende maximal fünf Sätze und fasse die Antwort kurz zusammen.\"\n",
    "        f\"Frage: {prompt}\"\n",
    "        f\"Kontext: {context}\"\n",
    "        \"Antwort: \" \n",
    "        )\n",
    "\n",
    "    simple_system_prompt =  \"Du bist ein Assistent für die Beantwortung von Fragen bezüglich Plenarsitzungen des Deutschen Bundestags.\"\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=simple_system_prompt),\n",
    "        HumanMessage(\n",
    "            content=simple_rag_prompt\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    answer = chat_model.invoke(messages).content\n",
    "    return (answer, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self Fact-Checking\n",
    "def check_facts(answer:str, documents: list[Document])-> tuple[bool, str]:\n",
    "    context = format_docs(documents=documents)\n",
    "\n",
    "    fact_checking_prompt = (\n",
    "        \"Du hast die Aufgabe, herauszufinden, ob die Hypothese begründet ist und mit den Beweisen übereinstimmt. \"\n",
    "        \"Verwende nur den Inhalt der Beweise und stütze dich nicht auf externes Wissen. \"\n",
    "        f\"Antworte mit ja/nein. Beweise: {context} \"\n",
    "        f\"Hypothese: {answer}: \"\n",
    "        \"Antwort: \"\n",
    "    )\n",
    "\n",
    "    ai_msg = fact_check_model.invoke([\n",
    "        HumanMessage(\n",
    "            content=fact_checking_prompt\n",
    "        ),\n",
    "    ]).content\n",
    "    is_okay = ai_msg.lower().strip().startswith(\"ja\")\n",
    "    return (is_okay, ai_msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "Fact-Check: OKAY\n",
      "Stephan Brandner (AfD) nannte Herrn Steffen einen \"Hetzer\".\n"
     ]
    }
   ],
   "source": [
    "# Ask RAG\n",
    "prompt = \"Wer nannte Herrn Steffen einen Hetzer?\"\n",
    "(answer, documents) = get_rag_answer(prompt=prompt)\n",
    "(is_based_on_facts, fact_checking_answer) = check_facts(answer=answer, documents=documents)\n",
    "\n",
    "print(f\"Fact-Check: {'OKAY' if is_based_on_facts else 'NOT OKAY'}\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Ja, die Hypothese wird bestätigt. Aus dem Stenografischen Bericht geht hervor, dass Stephan Brandner (AfD) Herrn Steffen als \"Hetzer\" bezeichnet hat.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact_checking_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Evaluation Dataset\n",
    "\n",
    "evaluation_questions = [\n",
    "    \"Wer nannte Herrn Steffen einen Hetzer?\"\n",
    "]\n",
    "\n",
    "evaluation_answers = [\n",
    "    \"Stephan Brandner (AfD) nannte Herrn Steffen einen 'Hetzer'.\"\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
